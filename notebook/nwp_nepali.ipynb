{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuBZpp23DkPp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"01.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines)\n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces\n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "T5L_G4SDDq4M",
        "outputId": "3f6c3734-f0e6-4d74-ff59-4da3e707abb0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"राष्ट्रिय लीग जावलाखेलको कठिन जित\"\"दुई महिना बितेको छैन आर्मीले सामसुङ जावलाखेललाई सफल पोखरा कपको पहिलो खेलमै ले पराजित गरेको उही पोखरा रंगशालामा शनिबार जावलाखेलले मीठो बदला लिएको छ \"\" दुई महिना बितेको छैन आर्मीले सामसुङ जावलाखेललाई सफल पोखरा कपको पहिलो खेलमै ले पराजित गरेको उही पोखरा रंगशालामा शनिबार जावलाखेलले मीठो बदला लिएको छ जावलाखेलले आर्मीलाई ले हराएर रेडबुल ए डिभिजन राष्ट्रिय लिगमा राम्रो सुरुआत गर्यो केन्याली फरवार्ड जेरार्ड मुकोन्जाले औं मिनेटमा गरेको गोल पोखरा कपको उपविजेता आर्मीविरु'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhDOuUxjDyOD",
        "outputId": "af92687d-1535-478c-d0f4-1755c9a2f4af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 1535, 228, 356, 25, 22, 656, 2536, 75, 314, 185, 396, 124, 193, 516]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "sequences = []\n",
        "# when i=3 slicing will be from 0:4 where 3 words are appended\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qBTKiONDz5c",
        "outputId": "26d70a35-79b1-4723-cc5b-6b7faf4d236a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7962\n",
            "The Length of sequences are:  42550\n",
            "Data:  [[  10 1535  228]\n",
            " [1535  228  356]\n",
            " [ 228  356   25]\n",
            " [ 356   25   22]\n",
            " [  25   22  656]\n",
            " [  22  656 2536]\n",
            " [ 656 2536   75]\n",
            " [2536   75  314]\n",
            " [  75  314  185]\n",
            " [ 314  185  396]]\n",
            "Response:  [ 356   25   22  656 2536   75  314  185  396  124]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsEFffuPEASW",
        "outputId": "17949c40-1ceb-4dd8-cbf3-9ad085ffa635"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             79620     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 256)            273408    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 7962)              2046234   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2924574 (11.16 MB)\n",
            "Trainable params: 2924574 (11.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='plot.png', show_layer_names=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "GPYybLn5ECF5",
        "outputId": "8c748abc-a7fb-415a-f2aa-e4e78005c7b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsMR1DlvEEKn",
        "outputId": "28a0198b-ab30-459b-8931-9d5534fc3d4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 7.9936 - accuracy: 0.0177\n",
            "Epoch 1: loss improved from inf to 7.99360, saving model to next_words.h5\n",
            "665/665 [==============================] - 18s 17ms/step - loss: 7.9936 - accuracy: 0.0177\n",
            "Epoch 2/50\n",
            "  1/665 [..............................] - ETA: 9s - loss: 7.7016 - accuracy: 0.0156"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "665/665 [==============================] - ETA: 0s - loss: 7.5697 - accuracy: 0.0242\n",
            "Epoch 2: loss improved from 7.99360 to 7.56966, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 7.5697 - accuracy: 0.0242\n",
            "Epoch 3/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 7.3624 - accuracy: 0.0304\n",
            "Epoch 3: loss improved from 7.56966 to 7.36238, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 7.3624 - accuracy: 0.0304\n",
            "Epoch 4/50\n",
            "662/665 [============================>.] - ETA: 0s - loss: 7.1068 - accuracy: 0.0366\n",
            "Epoch 4: loss improved from 7.36238 to 7.10539, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 7.1054 - accuracy: 0.0366\n",
            "Epoch 5/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 6.7999 - accuracy: 0.0446\n",
            "Epoch 5: loss improved from 7.10539 to 6.79985, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 6.7999 - accuracy: 0.0446\n",
            "Epoch 6/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 6.4358 - accuracy: 0.0551\n",
            "Epoch 6: loss improved from 6.79985 to 6.43582, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 6.4358 - accuracy: 0.0551\n",
            "Epoch 7/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 6.0248 - accuracy: 0.0713\n",
            "Epoch 7: loss improved from 6.43582 to 6.02483, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 6.0248 - accuracy: 0.0713\n",
            "Epoch 8/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 5.6046 - accuracy: 0.0873\n",
            "Epoch 8: loss improved from 6.02483 to 5.60480, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 5.6048 - accuracy: 0.0872\n",
            "Epoch 9/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 5.1914 - accuracy: 0.1079\n",
            "Epoch 9: loss improved from 5.60480 to 5.19141, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 5.1914 - accuracy: 0.1079\n",
            "Epoch 10/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 4.7951 - accuracy: 0.1366\n",
            "Epoch 10: loss improved from 5.19141 to 4.79587, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 4.7959 - accuracy: 0.1365\n",
            "Epoch 11/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 4.4342 - accuracy: 0.1699\n",
            "Epoch 11: loss improved from 4.79587 to 4.43458, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 4.4346 - accuracy: 0.1699\n",
            "Epoch 12/50\n",
            "662/665 [============================>.] - ETA: 0s - loss: 4.0979 - accuracy: 0.2091\n",
            "Epoch 12: loss improved from 4.43458 to 4.09801, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 4.0980 - accuracy: 0.2091\n",
            "Epoch 13/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 3.7967 - accuracy: 0.2536\n",
            "Epoch 13: loss improved from 4.09801 to 3.79652, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 3.7965 - accuracy: 0.2535\n",
            "Epoch 14/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 3.5135 - accuracy: 0.3035\n",
            "Epoch 14: loss improved from 3.79652 to 3.51298, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 3.5130 - accuracy: 0.3036\n",
            "Epoch 15/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 3.2583 - accuracy: 0.3475\n",
            "Epoch 15: loss improved from 3.51298 to 3.25833, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 3.2583 - accuracy: 0.3475\n",
            "Epoch 16/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 3.0239 - accuracy: 0.3902\n",
            "Epoch 16: loss improved from 3.25833 to 3.02482, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 3.0248 - accuracy: 0.3899\n",
            "Epoch 17/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 2.8142 - accuracy: 0.4282\n",
            "Epoch 17: loss improved from 3.02482 to 2.81430, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 2.8143 - accuracy: 0.4282\n",
            "Epoch 18/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 2.6206 - accuracy: 0.4661\n",
            "Epoch 18: loss improved from 2.81430 to 2.62109, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 2.6211 - accuracy: 0.4660\n",
            "Epoch 19/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 2.4438 - accuracy: 0.4977\n",
            "Epoch 19: loss improved from 2.62109 to 2.44523, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 2.4452 - accuracy: 0.4974\n",
            "Epoch 20/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 2.2824 - accuracy: 0.5300\n",
            "Epoch 20: loss improved from 2.44523 to 2.28387, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 2.2839 - accuracy: 0.5298\n",
            "Epoch 21/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 2.1339 - accuracy: 0.5577\n",
            "Epoch 21: loss improved from 2.28387 to 2.13425, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 2.1343 - accuracy: 0.5577\n",
            "Epoch 22/50\n",
            "662/665 [============================>.] - ETA: 0s - loss: 1.9923 - accuracy: 0.5861\n",
            "Epoch 22: loss improved from 2.13425 to 1.99340, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.9934 - accuracy: 0.5859\n",
            "Epoch 23/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 1.8680 - accuracy: 0.6108\n",
            "Epoch 23: loss improved from 1.99340 to 1.86817, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 1.8682 - accuracy: 0.6107\n",
            "Epoch 24/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 1.7452 - accuracy: 0.6360\n",
            "Epoch 24: loss improved from 1.86817 to 1.74517, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.7452 - accuracy: 0.6360\n",
            "Epoch 25/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 1.6370 - accuracy: 0.6564\n",
            "Epoch 25: loss improved from 1.74517 to 1.63678, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 1.6368 - accuracy: 0.6565\n",
            "Epoch 26/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 1.5302 - accuracy: 0.6797\n",
            "Epoch 26: loss improved from 1.63678 to 1.53016, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.5302 - accuracy: 0.6797\n",
            "Epoch 27/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 1.4367 - accuracy: 0.6972\n",
            "Epoch 27: loss improved from 1.53016 to 1.43691, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 1.4369 - accuracy: 0.6972\n",
            "Epoch 28/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 1.3436 - accuracy: 0.7179\n",
            "Epoch 28: loss improved from 1.43691 to 1.34369, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.3437 - accuracy: 0.7179\n",
            "Epoch 29/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 1.2595 - accuracy: 0.7356\n",
            "Epoch 29: loss improved from 1.34369 to 1.25928, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 1.2593 - accuracy: 0.7356\n",
            "Epoch 30/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 1.1846 - accuracy: 0.7499\n",
            "Epoch 30: loss improved from 1.25928 to 1.18457, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.1846 - accuracy: 0.7499\n",
            "Epoch 31/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 1.1128 - accuracy: 0.7630\n",
            "Epoch 31: loss improved from 1.18457 to 1.11284, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 1.1128 - accuracy: 0.7630\n",
            "Epoch 32/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 1.0445 - accuracy: 0.7768\n",
            "Epoch 32: loss improved from 1.11284 to 1.04523, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 1.0452 - accuracy: 0.7767\n",
            "Epoch 33/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 0.9787 - accuracy: 0.7902\n",
            "Epoch 33: loss improved from 1.04523 to 0.97909, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.9791 - accuracy: 0.7900\n",
            "Epoch 34/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 0.9242 - accuracy: 0.8010\n",
            "Epoch 34: loss improved from 0.97909 to 0.92421, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.9242 - accuracy: 0.8009\n",
            "Epoch 35/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 0.8631 - accuracy: 0.8150\n",
            "Epoch 35: loss improved from 0.92421 to 0.86290, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 0.8629 - accuracy: 0.8150\n",
            "Epoch 36/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.8231\n",
            "Epoch 36: loss improved from 0.86290 to 0.81357, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 0.8136 - accuracy: 0.8231\n",
            "Epoch 37/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 0.7672 - accuracy: 0.8347\n",
            "Epoch 37: loss improved from 0.81357 to 0.76751, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.7675 - accuracy: 0.8345\n",
            "Epoch 38/50\n",
            "662/665 [============================>.] - ETA: 0s - loss: 0.7212 - accuracy: 0.8429\n",
            "Epoch 38: loss improved from 0.76751 to 0.72114, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 0.7211 - accuracy: 0.8428\n",
            "Epoch 39/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.8507\n",
            "Epoch 39: loss improved from 0.72114 to 0.68370, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.6837 - accuracy: 0.8507\n",
            "Epoch 40/50\n",
            "661/665 [============================>.] - ETA: 0s - loss: 0.6433 - accuracy: 0.8596\n",
            "Epoch 40: loss improved from 0.68370 to 0.64343, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 0.6434 - accuracy: 0.8597\n",
            "Epoch 41/50\n",
            "662/665 [============================>.] - ETA: 0s - loss: 0.6057 - accuracy: 0.8658\n",
            "Epoch 41: loss improved from 0.64343 to 0.60580, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.6058 - accuracy: 0.8658\n",
            "Epoch 42/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.5749 - accuracy: 0.8719\n",
            "Epoch 42: loss improved from 0.60580 to 0.57488, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 0.5749 - accuracy: 0.8719\n",
            "Epoch 43/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 0.5439 - accuracy: 0.8765\n",
            "Epoch 43: loss improved from 0.57488 to 0.54390, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.5439 - accuracy: 0.8765\n",
            "Epoch 44/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 0.5152 - accuracy: 0.8848\n",
            "Epoch 44: loss improved from 0.54390 to 0.51608, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 10ms/step - loss: 0.5161 - accuracy: 0.8844\n",
            "Epoch 45/50\n",
            "664/665 [============================>.] - ETA: 0s - loss: 0.4945 - accuracy: 0.8871\n",
            "Epoch 45: loss improved from 0.51608 to 0.49463, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.4946 - accuracy: 0.8871\n",
            "Epoch 46/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.8932\n",
            "Epoch 46: loss improved from 0.49463 to 0.46836, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 0.4684 - accuracy: 0.8932\n",
            "Epoch 47/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.4486 - accuracy: 0.8964\n",
            "Epoch 47: loss improved from 0.46836 to 0.44861, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.4486 - accuracy: 0.8964\n",
            "Epoch 48/50\n",
            "665/665 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.9003\n",
            "Epoch 48: loss improved from 0.44861 to 0.42852, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 0.4285 - accuracy: 0.9003\n",
            "Epoch 49/50\n",
            "663/665 [============================>.] - ETA: 0s - loss: 0.4109 - accuracy: 0.9028\n",
            "Epoch 49: loss improved from 0.42852 to 0.41082, saving model to next_words.h5\n",
            "665/665 [==============================] - 6s 9ms/step - loss: 0.4108 - accuracy: 0.9028\n",
            "Epoch 50/50\n",
            "660/665 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.9069\n",
            "Epoch 50: loss improved from 0.41082 to 0.39273, saving model to next_words.h5\n",
            "665/665 [==============================] - 7s 10ms/step - loss: 0.3927 - accuracy: 0.9069\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d9ca8896ec0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def Predict_Next_Words(model, tokenizer, text, top_n=5):\n",
        "    # Check if the text contains numeric values\n",
        "    is_numeric = all(word.isnumeric() for word in text)\n",
        "\n",
        "    if is_numeric:\n",
        "        # Input is a sequence of indices\n",
        "        text_sequence = [int(word) for word in text]\n",
        "    else:\n",
        "        # Input is a sequence of words\n",
        "        context_words = text + [''] * (2 - len(text))\n",
        "        text_sequence = tokenizer.texts_to_sequences([context_words])[0]\n",
        "\n",
        "    sequence = pad_sequences([text_sequence], maxlen=3, padding='pre', truncating='post')[0]\n",
        "    sequence = np.array(sequence)\n",
        "\n",
        "    preds = model.predict(sequence.reshape(1, -1))\n",
        "    top_preds_indices = np.argsort(preds[0])[::-1][:top_n]\n",
        "\n",
        "    predicted_words = []\n",
        "\n",
        "    for index in top_preds_indices:\n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == index:\n",
        "                predicted_word = key\n",
        "                predicted_words.append(predicted_word)\n",
        "                break\n",
        "\n",
        "    print(predicted_words)\n",
        "    return predicted_words\n"
      ],
      "metadata": {
        "id": "IjOZltP1EFiv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    text = input(\"Enter your line: \")\n",
        "\n",
        "    if text == \"0\":\n",
        "        print(\"Execution completed.....\")\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-3:]\n",
        "            print(text)\n",
        "\n",
        "            Predict_Next_Words(model, tokenizer, text, top_n=5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred: \", e)\n",
        "            continue\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRUkuJSqEToT",
        "outputId": "ef824f18-adc6-41f5-fecd-d6d69b3ffe6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: एसियाली\n",
            "['एसियाली']\n",
            "1/1 [==============================] - 1s 686ms/step\n",
            "['सबै', 'पुलिस', 'खेलाडी', 'भने', 'वषर्ीय']\n",
            "Enter your line: कतारी क्लब अल साद\n",
            "['क्लब', 'अल', 'साद']\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "['एसियाको', 'कपको', 'त्यसैले', 'नेपाली', 'शैलीमा']\n",
            "Enter your line: नेपाल सुपर लिग फुटबल\n",
            "['सुपर', 'लिग', 'फुटबल']\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "['तेस्रो', 'पनि', 'पेनाल्टी', 'राष्ट्रिय', 'उनले']\n",
            "Enter your line: पहिलो हाफमा चितवनका विमल\n",
            "['हाफमा', 'चितवनका', 'विमल']\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "['प्रशिक्षक', 'र', 'लागि', 'राम्रो', 'केही']\n",
            "Enter your line: तिमिलाइ   कस्तो \n",
            "['', 'कस्तो', '']\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "['सबै', 'लामाले', 'फाइनलमा', 'प्रशिक्षक', 'राम्रो']\n",
            "Enter your line: 0\n",
            "Execution completed.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohkSMlglEXjs"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}